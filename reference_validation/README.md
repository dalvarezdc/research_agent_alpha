# Reference Validation System

A comprehensive reference validation system for verifying citations, sources, and URLs across all medical AI agents.

## Overview

The Reference Validation System addresses the **critical problem** of ensuring that references generated by LLMs are real, accessible, and credible. It validates references by:

1. **Verifying they exist** - Checks DOI, PMID, arXiv IDs, and URLs
2. **Assessing credibility** - Scores references based on verifiability
3. **Extracting metadata** - Pulls authors, years, journals
4. **Caching results** - Improves performance with intelligent caching

**Priority:** Existence verification over authority verification

## Quick Start

```python
from reference_validation import ReferenceValidator, ValidationConfig

# Initialize validator
validator = ReferenceValidator()

# Validate a single reference
result = validator.validate_reference(
    "Smith J et al. (2020). Important Research. Nature. DOI: 10.1038/example"
)

print(f"Valid: {result.is_valid}")
print(f"Credibility Score: {result.credibility_score}/100")
print(f"DOI Verified: {result.doi_valid}")
print(f"PubMed Verified: {result.pubmed_verified}")
```

## Architecture

### Single Unified Validator

The system uses a **unified validator** architecture instead of multiple separate validators:

```
ReferenceValidator (Main Interface)
    │
    ├── CitationValidator (format parsing)
    ├── URLChecker (accessibility)
    ├── UnifiedReferenceValidator (DOI/PMID/arXiv verification)
    ├── ReferenceExtractor (extract from text)
    └── ScoringEngine (aggregate results)
```

### Why Unified?

- **Simpler** - One validator instead of many classes
- **Maintainable** - All logic in one place
- **Extensible** - Easy to add new identifier types
- **Efficient** - Shared rate limiting and caching

## Features

### 1. Multi-Level Validation

```python
from reference_validation import ValidationLevel

# Quick: Format check + cache lookup (~50ms)
result = validator.validate_reference(citation, validation_level=ValidationLevel.QUICK)

# Standard: Format + identifier verification (~500ms)
result = validator.validate_reference(citation, validation_level=ValidationLevel.STANDARD)

# Thorough: All checks including URL accessibility (~5s)
result = validator.validate_reference(citation, validation_level=ValidationLevel.THOROUGH)
```

### 2. Batch Validation

```python
citations = [
    "Paper 1. DOI: 10.1234/example1",
    "Paper 2. PMID: 12345678",
    "Paper 3. https://example.com/paper3"
]

report = validator.validate_batch(citations, level=ValidationLevel.STANDARD)

print(f"Overall Score: {report.overall_score}/100")
print(f"Valid: {report.valid_references}/{report.total_references}")
print(f"Recommendations: {report.recommendations}")
```

### 3. Intelligent Caching

```python
# Configure cache
config = ValidationConfig(
    cache_backend="sqlite",  # or "json", "memory", "none"
    cache_ttl_days=30,
    cache_path="./cache/validation.db"
)

validator = ReferenceValidator(config)
```

Cache backends:
- **sqlite** (default) - Persistent, fast, no external dependencies
- **json** - Persistent, human-readable, good for debugging
- **memory** - Fast, temporary, good for testing
- **none** - No caching

### 4. Reference Extraction

```python
# Extract references from text
text = """
This study found X [1] and Y [2].

## References
[1] Smith et al. (2020). Paper title. DOI: 10.1234/example
[2] Jones et al. (2021). Another paper. PMID: 12345678
"""

references = validator.extract_references(text)

for ref in references:
    print(f"Found: {ref.raw_text}")
    print(f"Context: {ref.context}")
```

### 5. Identifier Support

The system automatically detects and validates:

- **DOI** - via doi.org resolver and CrossRef API
- **PMID** - via PubMed E-utilities API
- **arXiv ID** - via arXiv API
- **URLs** - via accessibility checks

Priority order: PMID > DOI > arXiv > URL

## Configuration

```python
from reference_validation import ValidationConfig, ValidationLevel

config = ValidationConfig(
    # Cache settings
    cache_backend="sqlite",
    cache_ttl_days=30,
    cache_path="./cache/validation.db",

    # Validation behavior
    validation_level=ValidationLevel.STANDARD,
    timeout_seconds=10,
    retry_attempts=3,

    # API settings
    enable_pubmed=True,
    enable_crossref=True,
    pubmed_api_key="your_key",  # Optional, for higher rate limits
    pubmed_email="your@email.com",

    # Thresholds
    min_credibility_score=60,
    require_peer_review=False,

    # Logging
    enable_logging=True,
    log_level="INFO"
)

validator = ReferenceValidator(config)
```

## Integration with Agents

### Example: Medication Analyzer

```python
from medical_procedure_analyzer.medication_analyzer import MedicationAnalyzer
from reference_validation import ReferenceValidator, ValidationConfig

# Initialize analyzer
analyzer = MedicationAnalyzer()

# Initialize validator with high standards for medical data
validator = ReferenceValidator(ValidationConfig(
    min_credibility_score=85,  # High threshold for drug data
    require_peer_review=True,
    validation_level=ValidationLevel.THOROUGH
))

# Run analysis
result = analyzer.analyze_medication(...)

# Validate all references in result
validation_report = validator.validate_analysis(result)

# Attach validation report
result.validation_report = validation_report
result.overall_credibility = validation_report.overall_score

# Check if meets standards
if validation_report.overall_score < 85:
    result.warnings.append(
        "⚠️ Some references have low credibility. Verify independently."
    )
```

### Example: Real-Time Validation During Generation

```python
def _phase1_evidence_gathering(self, input_data):
    # Generate evidence
    evidence = self.llm.generate(prompt)

    # Extract and validate references in real-time
    refs = self.validator.extract_references(evidence)
    citations = [ref.raw_text for ref in refs]

    validation = self.validator.validate_batch(
        citations,
        level=ValidationLevel.QUICK  # Fast for real-time
    )

    # Filter low-quality sources
    if validation.overall_score < 70:
        # Regenerate with feedback
        feedback = f"Some sources have low credibility. Use peer-reviewed sources."
        evidence = self.llm.generate(prompt, feedback=feedback)

    return evidence, validation
```

## Validation Result Structure

```python
result = validator.validate_reference(citation)

# Overall status
result.is_valid          # bool
result.credibility_score # 0-100
result.confidence        # 0-1

# Verification status
result.citation_format_valid  # Format check passed
result.doi_valid             # DOI exists
result.pubmed_verified       # Found in PubMed
result.url_accessible        # URL is accessible

# Extracted metadata
result.doi                   # Extracted DOI
result.pmid                  # Extracted PMID
result.publication_year      # Year published
result.journal_name          # Journal name
result.authors               # List of authors
result.peer_reviewed         # Is peer-reviewed

# Issues and recommendations
result.issues                # List of ValidationIssue
result.warnings              # List of warnings
result.recommendations       # Suggested improvements

# Performance
result.validation_time_ms    # Time taken
result.cache_hit            # Was cached
result.validators_used       # Which validators ran
```

## Credibility Scoring

The system calculates credibility scores (0-100) based on:

| Factor | Points | Notes |
|--------|--------|-------|
| Base score | 20 | For formatted text |
| Has DOI | +25 | Gold standard for verification |
| Has PMID | +25 | Excellent for medical research |
| Has URL (no DOI/PMID) | +15 | Good but less reliable |
| Has year | +15 | Helps verify accuracy |
| Has authors | +15 | Helps verify accuracy |
| Regulatory source | +10 | FDA, EMA, WHO, etc. |
| PubMed verified | +20 | Actually found in database |
| DOI verified | +15 | DOI resolves successfully |
| Reliable domain | +15 | Known trustworthy source |

**Maximum Score:** 100

## Performance

| Operation | Target Time | Notes |
|-----------|-------------|-------|
| Citation extraction | <100ms | Per reference |
| Quick validation (cached) | <10ms | Format check + cache hit |
| Standard validation (cached) | <10ms | Cache hit |
| Standard validation (uncached) | <500ms | With external API calls |
| Thorough validation | <5s | All checks + URL test |
| Batch (10 refs, cached) | <100ms | Parallel execution |
| Batch (10 refs, uncached) | <5s | Parallel with rate limiting |

## API Reference

### ReferenceValidator

Main interface for validation.

```python
validator = ReferenceValidator(config: Optional[ValidationConfig] = None)
```

**Methods:**

- `validate_reference(citation, expected_claim=None, validation_level=None) -> ValidationResult`
- `validate_batch(citations, level=None, parallel=True) -> ValidationReport`
- `validate_analysis(analysis_result, level=None) -> ValidationReport`
- `extract_references(text) -> List[ExtractedReference]`
- `clear_cache() -> None`
- `get_stats() -> dict`

### ValidationConfig

Configuration options.

```python
config = ValidationConfig(
    cache_backend="sqlite",
    cache_ttl_days=30,
    validation_level=ValidationLevel.STANDARD,
    timeout_seconds=10,
    ...
)
```

### ValidationLevel

Enum for validation thoroughness.

- `ValidationLevel.QUICK` - Format check + cache (50ms)
- `ValidationLevel.STANDARD` - Format + identifiers (500ms)
- `ValidationLevel.THOROUGH` - All checks (5s)

## Testing

```bash
# Run tests with pytest
pytest reference_validation/tests/ -v

# Run basic test directly
python reference_validation/tests/test_basic.py

# Run with coverage
pytest reference_validation/tests/ --cov=reference_validation
```

## Best Practices

### 1. Choose Appropriate Validation Level

- **QUICK** for real-time feedback during LLM generation
- **STANDARD** for final report validation (recommended)
- **THOROUGH** for critical medical claims

### 2. Use Caching

```python
# SQLite cache (recommended for production)
config = ValidationConfig(cache_backend="sqlite")

# Memory cache (good for testing)
config = ValidationConfig(cache_backend="memory")
```

### 3. Set Appropriate Thresholds

```python
# For critical medical information
config = ValidationConfig(
    min_credibility_score=85,
    require_peer_review=True
)

# For general information
config = ValidationConfig(
    min_credibility_score=60,
    require_peer_review=False
)
```

### 4. Handle Validation Failures Gracefully

```python
result = validator.validate_reference(citation)

if not result.is_valid:
    print(f"⚠️ Validation issues:")
    for issue in result.issues:
        print(f"  - {issue.message}")
        if issue.recommendation:
            print(f"    Recommendation: {issue.recommendation}")
```

### 5. Use Batch Validation for Performance

```python
# Instead of:
for citation in citations:
    result = validator.validate_reference(citation)

# Do this:
report = validator.validate_batch(citations)
for result in report.results:
    # Process result
```

## Limitations

1. **Cannot verify content accuracy** - Only verifies references exist, not that they support the claim
2. **API rate limits** - PubMed: 3 req/sec (10 with API key), respect limits
3. **Paywall detection** - Cannot access paywalled content
4. **Language support** - Currently focused on English sources
5. **Hallucination detection** - Cannot detect if LLM fabricated a plausible-looking citation

## Future Enhancements

- [ ] Claim-evidence alignment checking using NLP
- [ ] Support for non-English sources
- [ ] Integration with retraction databases
- [ ] Machine learning for credibility scoring
- [ ] Automatic alternative source suggestions
- [ ] Real-time monitoring of source updates

## API Keys

### PubMed API Key (Optional)

Increases rate limit from 3 to 10 requests per second.

1. Register at: https://www.ncbi.nlm.nih.gov/account/
2. Get API key from account settings
3. Configure:

```python
config = ValidationConfig(
    pubmed_api_key="your_key_here",
    pubmed_email="your@email.com"  # Required by NCBI
)
```

## Contributing

When adding new validators or features:

1. Follow existing patterns in `core/base_validator.py`
2. Add tests in `tests/`
3. Update this README
4. Follow repository coding standards in `README_FOR_LLM_DEVELOPMENT.md`

## License

Part of the Research Agent Alpha project.

## Support

For issues or questions, see the main repository documentation.
